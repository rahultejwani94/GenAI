{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a00c66e",
   "metadata": {},
   "source": [
    "# RNN Forward Propagation With Time\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Example Sentence\n",
    "We will use the sentence: **\"The food is good.\"**  \n",
    "Goal: Understand the **forward propagation** process in RNN.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Vocabulary and One-Hot Encoding\n",
    "Unique words (5 total):  \n",
    "- \"the\"  \n",
    "- \"food\"  \n",
    "- \"good\"  \n",
    "- \"bad\"  \n",
    "- \"not\"  \n",
    "\n",
    "One-hot encoding (vector length = 5):  \n",
    "- \"the\"  â†’ [1, 0, 0, 0, 0]  \n",
    "- \"food\" â†’ [0, 1, 0, 0, 0]  \n",
    "- \"good\" â†’ [0, 0, 1, 0, 0]  \n",
    "\n",
    "Sentence representation (ignoring stop words like \"is\"):  \n",
    "\n",
    "\"The\" â†’ [1, 0, 0, 0, 0]\n",
    "\"food\" â†’ [0, 1, 0, 0, 0]\n",
    "\"good\" â†’ [0, 0, 1, 0, 0]\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ RNN Input and Hidden Layer Details\n",
    "- Each time step `t` takes **one input vector**.  \n",
    "- Hidden layer has **3 neurons**.  \n",
    "- Feedback loop: hidden state from previous step is passed to the next step.\n",
    "\n",
    "Example:\n",
    "- At `t=1`, input `X11 = [1, 0, 0, 0, 0]` â†’ hidden state `O1`.  \n",
    "- At `t=2`, input `X12 = [0, 1, 0, 0, 0]` + feedback from `O1`.  \n",
    "- At `t=3`, input `X13 = [0, 0, 1, 0, 0]` + feedback from `O2`.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Forward Propagation Equations\n",
    "\n",
    "At **t=1**:  \n",
    "\\[\n",
    "O_1 = f(X_11*W + B_1)\n",
    "\\]  \n",
    "\n",
    "At **t=2**:  \n",
    "\\[\n",
    "O_2 = f(X_12*W + O_1W' + B_1)\n",
    "\\]  \n",
    "\n",
    "At **t=3**:  \n",
    "\\[\n",
    "O_3 = f(X_13*W + O_2W' + B_1)\n",
    "\\]  \n",
    "\n",
    "Where:  \n",
    "- `W` = input-to-hidden weights  \n",
    "- `W'` = hidden-to-hidden (feedback) weights  \n",
    "- `B1` = bias for hidden layer  \n",
    "- `f` = activation function (tanh or ReLU)  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Output Layer for Classification\n",
    "\n",
    "Final hidden state â†’ classification output.  \n",
    "\n",
    "For binary classification:  \n",
    "\\[\n",
    "y^ = \\sigma(O_t W_{out} + b_{out})\n",
    "\\]\n",
    "\n",
    "- `Ïƒ` = sigmoid activation  \n",
    "- `W_out`, `b_out` = output layer weights and bias  \n",
    "\n",
    "For multi-class classification â†’ **Softmax** instead of Sigmoid.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Trainable Parameters Calculation\n",
    "- Input-to-hidden weights: \\( 5 * 3 = 15 \\)  \n",
    "- Hidden-to-hidden weights: \\( 3 * 3 = 9 \\)  \n",
    "- Hidden-to-output weights: \\( 3 * 1 = 3 \\)  \n",
    "- Biases for hidden neurons: 3  \n",
    "- Bias for output neuron: 1  \n",
    "\n",
    "**Total parameters = 15 + 9 + 3 + 3 + 1 = 31**\n",
    "\n",
    "---\n",
    "\n",
    "### Diagram Overview\n",
    "![RNN Forward Propagation with time](images\\RNN_forward_propagation_with_time.png)\n",
    "\n",
    "### ðŸ”¹ Summary of Forward Propagation\n",
    "1. Convert words â†’ vectors (one-hot encoding).  \n",
    "2. Feed one vector per time step into RNN.  \n",
    "3. Hidden state = function of current input + previous hidden state.  \n",
    "4. Final hidden state â†’ output layer â†’ prediction.  \n",
    "5. Trainable parameters = Input-Hidden + Hidden-Hidden + Hidden-Output + Biases.  \n",
    "\n",
    "âœ… This process enables RNN to **capture sequential dependencies** (context from previous words).\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
