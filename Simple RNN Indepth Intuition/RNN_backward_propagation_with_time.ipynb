{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdd6846b",
   "metadata": {},
   "source": [
    "# Understanding Slope, Derivatives, and Gradient Descent\n",
    "\n",
    "### 1. Slope\n",
    "- Slope tells us **how steep a line is**.  \n",
    "- In a straight line equation `y = mx + c`,  \n",
    "  - `m` = slope  \n",
    "  - It means: **change in y / change in x (Δy/Δx)**.  \n",
    "- Example: If slope = 2, then for every +1 change in `x`, `y` increases by +2.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. Derivatives\n",
    "- Derivative is a generalization of slope for **any curve** (not just straight lines).  \n",
    "- It tells us: **rate of change of a function w.r.t. its variable**.  \n",
    "- Example:  \n",
    "  - For `y = x²`, derivative = `dy/dx = 2x`.  \n",
    "  - At `x = 3`, slope of curve = `2(3) = 6`.  \n",
    "\n",
    "---\n",
    "\n",
    "### 3. Partial Derivatives\n",
    "- Used when function has **multiple variables**.  \n",
    "- Example: `f(x, y) = x² + y²`  \n",
    "  - Partial derivative wrt `x`: `∂f/∂x = 2x` (treat `y` as constant)  \n",
    "  - Partial derivative wrt `y`: `∂f/∂y = 2y` (treat `x` as constant)  \n",
    "- In Machine Learning, cost functions depend on **many weights**, so we use partial derivatives.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Linear vs Non-linear Case\n",
    "- **Linear case**: slope is constant (e.g., straight line).  \n",
    "- **Non-linear case**: slope keeps changing at every point, so we use **derivatives** to calculate slope dynamically.  \n",
    "\n",
    "---\n",
    "\n",
    "### 5. Gradient Descent\n",
    "- Gradient = vector of **all partial derivatives**.  \n",
    "- Descent = moving in the direction of **negative gradient** (downhill).  \n",
    "- Why? Because we want to minimize the **loss function**.  \n",
    "\n",
    "**Steps of Gradient Descent:**\n",
    "1. Start with random weights.  \n",
    "2. Compute the loss (error).  \n",
    "3. Calculate derivatives (slopes).  \n",
    "4. Update weights:  \n",
    "   `new_weight = old_weight - learning_rate * derivative`  \n",
    "5. Repeat until convergence.  \n",
    "\n",
    "---\n",
    "\n",
    "✅ Example in ML:  \n",
    "- Suppose we are predicting house prices.  \n",
    "- Loss function = Mean Squared Error (MSE).  \n",
    "- Gradient descent helps us adjust weights (like `size`, `rooms`) to minimize error.  \n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "\n",
    "# RNN Backpropagation Through Time (BPTT)\n",
    "\n",
    "Previously, we explored the forward propagation process and the basic architecture of a simple RNN. Now, after computing the **loss function** at the end of the forward pass, our goal is to **reduce this loss by updating the weights through backpropagation**.\n",
    "\n",
    "---\n",
    "### Diagram Overview\n",
    "![RNN Backward Propagation with time](images\\RNN_backward_propagation_with_time.png)\n",
    "\n",
    "---\n",
    "\n",
    "## Loss Calculation and Objective\n",
    "After obtaining the predicted output $\\hat{y}$, we calculate the loss by comparing it with the true label $y$.  \n",
    "\n",
    "Our objective is to minimize this loss by updating the weights:\n",
    "- $w_I$ (input weights)\n",
    "- $w_H$ (hidden weights)\n",
    "- $w_O$ (output weights)\n",
    "\n",
    "---\n",
    "\n",
    "## Weight Update Formula\n",
    "The general weight update rule using gradient descent is:\n",
    "\n",
    "$$\n",
    "w_{new} = w_{old} - \\eta \\times \\frac{\\partial Loss}{\\partial w_{old}}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $w_{old}$ = current weight  \n",
    "- $w_{new}$ = updated weight  \n",
    "- $\\eta$ = learning rate  \n",
    "- $\\frac{\\partial Loss}{\\partial w_{old}}$ = gradient of the loss w.r.t. the weight  \n",
    "\n",
    "---\n",
    "\n",
    "## Updating Output Weights ($w_O$)\n",
    "\n",
    "Using the chain rule:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial Loss}{\\partial w_O} = \\frac{\\partial Loss}{\\partial \\hat{y}} \\times \\frac{\\partial \\hat{y}}{\\partial w_O}\n",
    "$$\n",
    "\n",
    "Update rule:\n",
    "\n",
    "$$\n",
    "w_O^{new} = w_O^{old} - \\eta \\times \\frac{\\partial Loss}{\\partial w_O}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Updating Hidden Weights ($w_H$)\n",
    "\n",
    "The hidden weights are shared across all time steps, so we **sum gradients over all $t$**.\n",
    "\n",
    "- At $t=3$:  \n",
    "$$\n",
    "\\frac{\\partial Loss}{\\partial w_H} = \n",
    "\\frac{\\partial Loss}{\\partial \\hat{y}} \\times \n",
    "\\frac{\\partial \\hat{y}}{\\partial o_3} \\times \n",
    "\\frac{\\partial o_3}{\\partial w_H}\n",
    "$$\n",
    "\n",
    "- At $t=2$:  \n",
    "$$\n",
    "\\frac{\\partial Loss}{\\partial w_H} += \n",
    "\\frac{\\partial Loss}{\\partial \\hat{y}} \\times \n",
    "\\frac{\\partial \\hat{y}}{\\partial o_3} \\times \n",
    "\\frac{\\partial o_3}{\\partial o_2} \\times \n",
    "\\frac{\\partial o_2}{\\partial w_H}\n",
    "$$\n",
    "\n",
    "- At $t=1$:  \n",
    "$$\n",
    "\\frac{\\partial Loss}{\\partial w_H} += \n",
    "\\frac{\\partial Loss}{\\partial \\hat{y}} \\times \n",
    "\\frac{\\partial \\hat{y}}{\\partial o_3} \\times \n",
    "\\frac{\\partial o_3}{\\partial o_2} \\times \n",
    "\\frac{\\partial o_2}{\\partial o_1} \\times \n",
    "\\frac{\\partial o_1}{\\partial w_H}\n",
    "$$\n",
    "\n",
    "Update rule:\n",
    "\n",
    "$$\n",
    "w_H^{new} = w_H^{old} - \\eta \\times \\frac{\\partial Loss}{\\partial w_H}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Updating Input Weights ($w_I$)\n",
    "\n",
    "Similar to $w_H$, but gradients flow from input:\n",
    "\n",
    "- At $t=3$:  \n",
    "$$\n",
    "\\frac{\\partial Loss}{\\partial w_I} = \n",
    "\\frac{\\partial Loss}{\\partial \\hat{y}} \\times \n",
    "\\frac{\\partial \\hat{y}}{\\partial o_3} \\times \n",
    "\\frac{\\partial o_3}{\\partial w_I}\n",
    "$$\n",
    "\n",
    "- At $t=2$:  \n",
    "$$\n",
    "\\frac{\\partial Loss}{\\partial w_I} += \n",
    "\\frac{\\partial Loss}{\\partial \\hat{y}} \\times \n",
    "\\frac{\\partial \\hat{y}}{\\partial o_3} \\times \n",
    "\\frac{\\partial o_3}{\\partial o_2} \\times \n",
    "\\frac{\\partial o_2}{\\partial w_I}\n",
    "$$\n",
    "\n",
    "- At $t=1$:  \n",
    "$$\n",
    "\\frac{\\partial Loss}{\\partial w_I} += \n",
    "\\frac{\\partial Loss}{\\partial \\hat{y}} \\times \n",
    "\\frac{\\partial \\hat{y}}{\\partial o_3} \\times \n",
    "\\frac{\\partial o_3}{\\partial o_2} \\times \n",
    "\\frac{\\partial o_2}{\\partial o_1} \\times \n",
    "\\frac{\\partial o_1}{\\partial w_I}\n",
    "$$\n",
    "\n",
    "Update rule:\n",
    "\n",
    "$$\n",
    "w_I^{new} = w_I^{old} - \\eta \\times \\frac{\\partial Loss}{\\partial w_I}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "# ✅ Summary\n",
    "- **Loss** is computed between predicted $\\hat{y}$ and true label $y$.  \n",
    "- **Weights ($w_I, w_H, w_O$)** are updated using **gradient descent**.  \n",
    "- **$w_O$** is updated directly since it connects to output.  \n",
    "- **$w_H$** and **$w_I$** require **Backpropagation Through Time (BPTT)**: gradients are summed across all time steps using the chain rule.  \n",
    "- Updates continue iteratively until loss converges (ideally to a global minimum).  \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
