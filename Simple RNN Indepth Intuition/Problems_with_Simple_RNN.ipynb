{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b779d8b",
   "metadata": {},
   "source": [
    "# Activation Functions, Their Ranges, and Derivatives\n",
    "\n",
    "| **Activation Function** | **Formula** | **Output Range** | **Derivative Range** | **Notes** |\n",
    "|--------------------------|-------------|------------------|-----------------------|-----------|\n",
    "| **Sigmoid (Logistic)** | $ \\sigma(x) = \\frac{1}{1 + e^{-x}} $ | (0, 1) | (0, 0.25) | Saturates at extremes → causes **vanishing gradients** |\n",
    "| **tanh (Hyperbolic Tangent)** | $ \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} $ | (-1, 1) | (0, 1), max = 1 at $x=0$ | Zero-centered → better than sigmoid |\n",
    "| **ReLU (Rectified Linear Unit)** | $ f(x) = \\max(0, x) $ | [0, ∞) | {0, 1} | Simple & efficient, but **dead ReLU problem** (neurons stuck at 0) |\n",
    "| **Leaky ReLU** | $ f(x) = \\max(\\alpha x, x), \\; \\alpha \\approx 0.01 $ | (-∞, ∞) | {α, 1} | Fixes dead ReLU by allowing small negative slope |\n",
    "| **ELU (Exponential Linear Unit)** | $ f(x) = \\begin{cases} x, & x > 0 \\\\ \\alpha(e^x - 1), & x \\leq 0 \\end{cases} $ | (-α, ∞) | (0, 1] | Smooth near zero, helps convergence |\n",
    "| **Softmax** | $ \\sigma(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}} $ | (0, 1), sums to 1 | Depends on input | Used in **classification output layer** |\n",
    "| **Swish** (Google) | $ f(x) = x \\cdot \\sigma(x) $ | (-∞, ∞) | ~ (0, 1) | Smooth & often better than ReLU |\n",
    "| **GELU (Gaussian Error Linear Unit)** | $ f(x) = x \\cdot \\Phi(x), \\;\\; \\Phi(x) = \\text{Gaussian CDF} $ | (-∞, ∞) | ~ (0, 1) | Default in **Transformers** (BERT, GPT) |\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Quick Insights\n",
    "- **Sigmoid/tanh** → prone to vanishing gradients (derivatives < 1).  \n",
    "- **ReLU family (ReLU, Leaky ReLU, ELU)** → reduce vanishing gradient, but ReLU can “die”.  \n",
    "- **Swish/GELU** → smooth, modern, widely used in deep models (esp. NLP).  \n",
    "- **Softmax** → not for hidden layers, but for **multi-class classification output**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88c72b8",
   "metadata": {},
   "source": [
    "# Problems with RNNs\n",
    "\n",
    "Now, we will understand some of the problems associated with RNNs.  \n",
    "This discussion will lead us to the next neural network variants, specifically **LSTM RNN** and **GRU RNN**, which address these issues.\n",
    "\n",
    "---\n",
    "\n",
    "## Weight Parameters in RNN\n",
    "In the RNN, we have several weight matrices:\n",
    "- **(W_I):** Input weight matrix  \n",
    "- **(W_H):** Hidden state weight matrix  \n",
    "- **(W_O):** Output weight matrix  \n",
    "\n",
    "These weights are shared across time steps and updated during training to minimize the loss.\n",
    "\n",
    "---\n",
    "\n",
    "## Short-Term Dependencies in RNN\n",
    "- Example: *\"The food is good.\"*  \n",
    "- The output depends on nearby words within a short context window.  \n",
    "- Simple RNNs handle this case well.\n",
    "\n",
    "---\n",
    "\n",
    "## Long-Term Dependencies and Challenges\n",
    "- Example: *\"Hey, my name is Krish, and I like sports like cricket and volleyball. I also like to make...\"*  \n",
    "- Prediction depends on words much earlier in the sequence.  \n",
    "- Simple RNNs **struggle** when sequence length grows large (e.g., 50–100 words).\n",
    "\n",
    "---\n",
    "\n",
    "## Vanishing Gradient Problem in RNN\n",
    "- Main issue: **Vanishing gradient problem**.  \n",
    "- During **backpropagation through time (BPTT)**, gradients are products of derivatives at each time step.  \n",
    "- Since derivatives < 1, repeated multiplication causes gradients to shrink exponentially → earlier steps have little influence.\n",
    "\n",
    "---\n",
    "\n",
    "## Diagram overview:\n",
    "\n",
    "![RNN problems](images\\Problems_with_RNN_1.png)\n",
    "\n",
    "![RNN problems](images\\Problems_with_RNN_2.png)\n",
    "\n",
    "---\n",
    "\n",
    "## Mathematical Illustration of Vanishing Gradient\n",
    "\n",
    "The derivative of the loss **L** with respect to the hidden weight **W_H** at time step **t=1**:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_H} = \\sum_{t=1}^T \n",
    "\\frac{\\partial L}{\\partial \\hat{y}} \\cdot \n",
    "\\frac{\\partial \\hat{y}}{\\partial O_T} \\cdot \n",
    "\\prod_{k=t+1}^T \\frac{\\partial O_k}{\\partial O_{k-1}} \\cdot \n",
    "\\frac{\\partial O_t}{\\partial W_H}\n",
    "$$\n",
    "\n",
    "Here, each term $ \\frac{\\partial O_k}{\\partial O_{k-1}} $ is typically less than one, causing the product to shrink exponentially as **T** increases.\n",
    "\n",
    "---\n",
    "\n",
    "## Impact of Activation Functions\n",
    "- **Sigmoid:** Derivatives between 0 and 0.25 → strong vanishing effect.  \n",
    "- **tanh:** Derivatives between 0 and 1 → still suffers for long sequences.  \n",
    "- **ReLU / Leaky ReLU:** Derivatives closer to 1 → helps reduce vanishing gradient.\n",
    "\n",
    "---\n",
    "\n",
    "## Mitigating Vanishing Gradient\n",
    "- Use **ReLU-based activations**.  \n",
    "- Use advanced architectures:  \n",
    "  - **LSTM (Long Short-Term Memory)**  \n",
    "  - **GRU (Gated Recurrent Unit)**  \n",
    "\n",
    "These introduce gating mechanisms to control information and preserve gradients.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "- **Problem:** Simple RNNs fail to capture long-term dependencies due to vanishing gradients.  \n",
    "- **Cause:** Repeated multiplication of small derivatives during BPTT.  \n",
    "- **Impact:** Earlier time steps have negligible influence.  \n",
    "- **Solution:** Use ReLU activations or advanced architectures like **LSTM** and **GRU** that handle long sequences effectively.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
