{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5df60248",
   "metadata": {},
   "source": [
    "# 📌 Overview of Neural Network Architecture\n",
    "\n",
    "An **Artificial Neural Network (ANN)** typically consists of:\n",
    "- An **input layer**\n",
    "- One or more **hidden layers**\n",
    "- An **output layer**\n",
    "\n",
    "👉 Example: A basic ANN might have **1 input layer, 2 hidden layers, and 1 output layer**.  \n",
    "Each layer contains **nodes (neurons)** interconnected with **weights** and **biases**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Example ANN Structure\n",
    "- **Input Layer**: 2 inputs  \n",
    "- **Hidden Layer 1**: 3 neurons  \n",
    "- **Hidden Layer 2**: 2 neurons  \n",
    "- **Output Layer**: 1 neuron  \n",
    "\n",
    "Connections between layers are represented by **weight matrices** and **bias vectors**.\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 Weight and Bias Dimensions\n",
    "\n",
    "1. **Input Layer (2 nodes) → Hidden Layer 1 (3 neurons)**\n",
    "   - Weight matrix: `2 × 3` → **6 weights**\n",
    "   - Bias terms: **3**\n",
    "\n",
    "2. **Hidden Layer 1 (3 neurons) → Hidden Layer 2 (2 neurons)**\n",
    "   - Weight matrix: `3 × 2` → **6 weights**\n",
    "   - Bias terms: **2**\n",
    "\n",
    "3. **Hidden Layer 2 (2 neurons) → Output Layer (1 neuron)**\n",
    "   - Weight matrix: `2 × 1` → **2 weights**\n",
    "   - Bias terms: **1**\n",
    "\n",
    "✅ **Total trainable parameters**  \n",
    "`6 + 3 + 6 + 2 + 2 + 1 = 20`\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ Building the ANN Model with TensorFlow Keras\n",
    "\n",
    "In **TensorFlow Keras**, the ANN is implemented as a **Sequential model** where layers are stacked one after another.  \n",
    "The `Dense` class is used to create **fully connected layers** (hidden neurons).\n",
    "\n",
    "### 🔑 Key Steps:\n",
    "1. **Initialize** a sequential model.  \n",
    "2. **Add Dense layers** with a specified number of neurons.  \n",
    "3. **Apply activation functions** (e.g., ReLU for hidden layers, sigmoid/softmax for output).  \n",
    "4. **Specify input shape** for the first hidden layer.  \n",
    "5. **Choose an optimizer** (e.g., Adam, SGD).  \n",
    "6. **Select a loss function** (e.g., binary crossentropy, categorical crossentropy).  \n",
    "7. **Define metrics** to evaluate model performance.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb4b22f",
   "metadata": {},
   "source": [
    "## Importing Required Libraries\n",
    "\n",
    "We import **TensorFlow** and relevant **Keras modules**:\n",
    "\n",
    "- `Sequential` from `tensorflow.keras.models`  \n",
    "  ➝ Used to initialize the Sequential model.  \n",
    "\n",
    "- `Dense` from `tensorflow.keras.layers`  \n",
    "  ➝ Used to create fully connected (dense) layers.  \n",
    "\n",
    "- `EarlyStopping` and `TensorBoard` from `tensorflow.keras.callbacks`  \n",
    "  ➝ For training control and visualization.  \n",
    "\n",
    "- `datetime`  \n",
    "  ➝ For timestamping log files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13324d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ANN Implementation\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e6ac27",
   "metadata": {},
   "source": [
    "### To run a notebook inside another notebook use\n",
    "\n",
    "```bash\n",
    "%run notebook1.ipynb\n",
    "```\n",
    "\n",
    "#### To use %run command we will need nbfromat library\n",
    "```bash\n",
    "pip install nbformat\n",
    "```\n",
    "\n",
    "- Now if we use %run notebook1.ipynb it will print all the outputs and the print statements in the notebook1.\n",
    "- if we don't want to see the output of notebook1 we can use following\n",
    "\n",
    "```bash\n",
    "%%capture cap\n",
    "%%run notebook1.ipynb\n",
    "\n",
    "```\n",
    "- capture will store the stdout and stderr in variable named cap. We can the see the output and the errors from notebook1 later using \n",
    "\n",
    "```python\n",
    "print(cap.stdout)\n",
    "print(cap.stderr)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cda5e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nbformat in c:\\users\\kunal.chugriya\\documents\\genai repo\\genai\\ann project implementation\\venv\\lib\\site-packages (5.10.4)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in c:\\users\\kunal.chugriya\\documents\\genai repo\\genai\\ann project implementation\\venv\\lib\\site-packages (from nbformat) (2.21.2)\n",
      "Requirement already satisfied: jsonschema>=2.6 in c:\\users\\kunal.chugriya\\documents\\genai repo\\genai\\ann project implementation\\venv\\lib\\site-packages (from nbformat) (4.25.1)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\kunal.chugriya\\documents\\genai repo\\genai\\ann project implementation\\venv\\lib\\site-packages (from nbformat) (5.8.1)\n",
      "Requirement already satisfied: traitlets>=5.1 in c:\\users\\kunal.chugriya\\documents\\genai repo\\genai\\ann project implementation\\venv\\lib\\site-packages (from nbformat) (5.14.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\kunal.chugriya\\documents\\genai repo\\genai\\ann project implementation\\venv\\lib\\site-packages (from jsonschema>=2.6->nbformat) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\kunal.chugriya\\documents\\genai repo\\genai\\ann project implementation\\venv\\lib\\site-packages (from jsonschema>=2.6->nbformat) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\kunal.chugriya\\documents\\genai repo\\genai\\ann project implementation\\venv\\lib\\site-packages (from jsonschema>=2.6->nbformat) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\kunal.chugriya\\documents\\genai repo\\genai\\ann project implementation\\venv\\lib\\site-packages (from jsonschema>=2.6->nbformat) (0.27.1)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\kunal.chugriya\\documents\\genai repo\\genai\\ann project implementation\\venv\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat) (4.4.0)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\kunal.chugriya\\documents\\genai repo\\genai\\ann project implementation\\venv\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat) (311)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in c:\\users\\kunal.chugriya\\documents\\genai repo\\genai\\ann project implementation\\venv\\lib\\site-packages (from referencing>=0.28.4->jsonschema>=2.6->nbformat) (4.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46a40fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap\n",
    "%run 2_preprocessing.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "837720f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 12)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0448fc9f",
   "metadata": {},
   "source": [
    "## Constructing the Sequential Model\n",
    "\n",
    "We initialize the **Sequential model** and add layers:\n",
    "\n",
    "- **First hidden layer**  \n",
    "  - 64 neurons  \n",
    "  - ReLU activation  \n",
    "  - Specifies the input shape based on the training data features  \n",
    "\n",
    "- **Second hidden layer**  \n",
    "  - 32 neurons  \n",
    "  - ReLU activation  \n",
    "\n",
    "- **Output layer**  \n",
    "  - 1 neuron  \n",
    "  - Sigmoid activation (for binary classification)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b341ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Kunal.Chugriya\\Documents\\GenAI Repo\\GenAI\\ANN Project Implementation\\venv\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## BUild ANN model\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),  ## first hidden layer woth 64 neurons, input_shape is only needed in the first hidden layer\n",
    "    Dense(32, activation='relu'), # HL2\n",
    "    Dense(1, activation='sigmoid')  ## output layer for binary classification\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebd8ccec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 64)                832       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2945 (11.50 KB)\n",
      "Trainable params: 2945 (11.50 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## this is used to get the summary of model like trainable parameters.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b31654",
   "metadata": {},
   "source": [
    "## Optimizer in Model Training (TensorFlow/Keras)\n",
    "\n",
    "An **optimizer** is an algorithm that updates the **weights and biases** of a neural network during training to minimize the **loss function**.\n",
    "\n",
    "👉 In simple terms:  \n",
    "- The **loss function** tells *how wrong* the model is.  \n",
    "- The **optimizer** decides *how to adjust the weights* to reduce that wrongness.  \n",
    "\n",
    "---\n",
    "\n",
    "### Common Optimizers in TensorFlow\n",
    "1. **SGD (Stochastic Gradient Descent)** – Basic optimizer, updates weights using gradients.  \n",
    "2. **Momentum** – Improves SGD by accelerating in relevant directions.  \n",
    "3. **Adam (Adaptive Moment Estimation)** – Most popular, adapts learning rate for each parameter.  \n",
    "4. **RMSprop** – Good for recurrent neural networks, adapts learning rate dynamically.  \n",
    "\n",
    "---\n",
    "\n",
    "### Example in Keras\n",
    "```python\n",
    "model.compile(\n",
    "    optimizer='adam',           # optimizer\n",
    "    loss='binary_crossentropy', # loss function\n",
    "    metrics=['accuracy']        # evaluation metric\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f61c2157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Kunal.Chugriya\\Documents\\GenAI Repo\\GenAI\\ANN Project Implementation\\venv\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## compile the model\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef411277",
   "metadata": {},
   "source": [
    "## TensorBoard\n",
    "\n",
    "**TensorBoard** is a visualization tool provided by TensorFlow.  \n",
    "It helps you **monitor, inspect, and debug** the training process of your neural networks.\n",
    "\n",
    "---\n",
    "\n",
    "### What TensorBoard Can Do\n",
    "1. **Track Metrics** – like loss, accuracy, precision, recall over time.  \n",
    "2. **Visualize Model Graphs** – see the layers and connections in your neural network.  \n",
    "3. **Histogram & Distributions** – monitor weight and bias changes during training.  \n",
    "4. **Embedding Projector** – visualize high-dimensional data (e.g., word embeddings).  \n",
    "5. **Scalars Dashboard** – see training/validation curves easily.\n",
    "\n",
    "---\n",
    "\n",
    "### Example Usage\n",
    "```python\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import datetime\n",
    "\n",
    "# Create a log directory with timestamp\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# Add callback during training\n",
    "model.fit(X_train, y_train, \n",
    "          epochs=10, \n",
    "          validation_data=(X_test, y_test), \n",
    "          callbacks=[tensorboard_callback])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f735c7f",
   "metadata": {},
   "source": [
    "## Setting Up TensorBoard and EarlyStopping Callbacks\n",
    "\n",
    "- **TensorBoard** is used to visualize training logs and metrics.  \n",
    "- **EarlyStopping** monitors validation loss and stops training if no improvement occurs for a specified number of epochs (`patience`).  \n",
    "- **EarlyStopping** is a technique used during model training to prevent overfitting.\n",
    "    - It monitors a chosen metric (like validation loss or accuracy).\n",
    "\n",
    "    - If the metric doesn’t improve for a set number of epochs (patience), training stops early.\n",
    "\n",
    "    - It can also restore the best weights achieved during training (so the model doesn’t end up with worse weights from later epochs).\n",
    "\n",
    "    - 👉 Example:\n",
    "        If you set patience=3 and validation loss hasn’t improved for 3 consecutive epochs, training stops automatically.\n",
    "\n",
    "---\n",
    "\n",
    "### Example Setup\n",
    "```python\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping\n",
    "import datetime\n",
    "\n",
    "# TensorBoard log directory\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# EarlyStopping to avoid overfitting\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\", \n",
    "    patience=3, \n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Add callbacks during training\n",
    "model.fit(X_train, y_train,\n",
    "          epochs=20,\n",
    "          validation_data=(X_test, y_test),\n",
    "          callbacks=[tensorboard_callback, early_stopping_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28c6a6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up tensorboard\n",
    "\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "log_dir = \"../logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorflow_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85d163d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting up Earlystopping\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9524452d",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "We train the model using **`model.fit()`** by providing:\n",
    "\n",
    "- **Training data**: `(X_train, y_train)`  \n",
    "- **Validation data**: `(X_test, y_test)` to monitor performance  \n",
    "- **Number of epochs**: e.g., `100`  \n",
    "- **Callbacks**: for TensorBoard and EarlyStopping  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "308af1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:From c:\\Users\\Kunal.Chugriya\\Documents\\GenAI Repo\\GenAI\\ANN Project Implementation\\venv\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Kunal.Chugriya\\Documents\\GenAI Repo\\GenAI\\ANN Project Implementation\\venv\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "250/250 [==============================] - 3s 8ms/step - loss: 0.4510 - accuracy: 0.8061 - val_loss: 0.4032 - val_accuracy: 0.8210\n",
      "Epoch 2/100\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.3980 - accuracy: 0.8363 - val_loss: 0.3730 - val_accuracy: 0.8465\n",
      "Epoch 3/100\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.3665 - accuracy: 0.8510 - val_loss: 0.3539 - val_accuracy: 0.8540\n",
      "Epoch 4/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3493 - accuracy: 0.8577 - val_loss: 0.3483 - val_accuracy: 0.8640\n",
      "Epoch 5/100\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.3415 - accuracy: 0.8608 - val_loss: 0.3439 - val_accuracy: 0.8585\n",
      "Epoch 6/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3354 - accuracy: 0.8631 - val_loss: 0.3443 - val_accuracy: 0.8585\n",
      "Epoch 7/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3303 - accuracy: 0.8627 - val_loss: 0.3439 - val_accuracy: 0.8585\n",
      "Epoch 8/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3286 - accuracy: 0.8634 - val_loss: 0.3476 - val_accuracy: 0.8605\n",
      "Epoch 9/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3250 - accuracy: 0.8659 - val_loss: 0.3458 - val_accuracy: 0.8595\n",
      "Epoch 10/100\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.3225 - accuracy: 0.8684 - val_loss: 0.3464 - val_accuracy: 0.8620\n",
      "Epoch 11/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3202 - accuracy: 0.8658 - val_loss: 0.3442 - val_accuracy: 0.8590\n",
      "Epoch 12/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3181 - accuracy: 0.8686 - val_loss: 0.3424 - val_accuracy: 0.8610\n",
      "Epoch 13/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3156 - accuracy: 0.8690 - val_loss: 0.3418 - val_accuracy: 0.8630\n",
      "Epoch 14/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3151 - accuracy: 0.8687 - val_loss: 0.3442 - val_accuracy: 0.8575\n",
      "Epoch 15/100\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.3133 - accuracy: 0.8687 - val_loss: 0.3377 - val_accuracy: 0.8640\n",
      "Epoch 16/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3111 - accuracy: 0.8690 - val_loss: 0.3456 - val_accuracy: 0.8580\n",
      "Epoch 17/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3102 - accuracy: 0.8680 - val_loss: 0.3445 - val_accuracy: 0.8600\n",
      "Epoch 18/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3095 - accuracy: 0.8710 - val_loss: 0.3446 - val_accuracy: 0.8615\n",
      "Epoch 19/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3082 - accuracy: 0.8692 - val_loss: 0.3472 - val_accuracy: 0.8540\n",
      "Epoch 20/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3076 - accuracy: 0.8719 - val_loss: 0.3461 - val_accuracy: 0.8575\n",
      "Epoch 21/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3054 - accuracy: 0.8714 - val_loss: 0.3518 - val_accuracy: 0.8545\n",
      "Epoch 22/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3060 - accuracy: 0.8698 - val_loss: 0.3494 - val_accuracy: 0.8605\n",
      "Epoch 23/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3037 - accuracy: 0.8727 - val_loss: 0.3426 - val_accuracy: 0.8595\n",
      "Epoch 24/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3008 - accuracy: 0.8714 - val_loss: 0.3493 - val_accuracy: 0.8600\n",
      "Epoch 25/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3011 - accuracy: 0.8733 - val_loss: 0.3429 - val_accuracy: 0.8600\n"
     ]
    }
   ],
   "source": [
    "## Train the model\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train, validation_data= (X_test, y_test), epochs = 100,\n",
    "    callbacks=[ tensorflow_callback, early_stopping_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893e8370",
   "metadata": {},
   "source": [
    "## Observing Training Results\n",
    "\n",
    "- Training stops early if **validation loss** does not improve for the specified **patience**.  \n",
    "- **Accuracy** and **loss metrics** are displayed per epoch.  \n",
    "- **TensorBoard logs** are saved for visualization.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52559591",
   "metadata": {},
   "source": [
    "## Saving the Trained Model\n",
    "\n",
    "- After training, the model can be **saved** to an `.h5` file.  \n",
    "- This format is compatible with **Keras** and allows the model to be **loaded later** for inference or further training.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4845f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kunal.Chugriya\\Documents\\GenAI Repo\\GenAI\\ANN Project Implementation\\venv\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# save the model\n",
    "model.save(\"../models/ann_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6c94d7",
   "metadata": {},
   "source": [
    "## Launching TensorBoard\n",
    "- To visualize training logs, load the TensorBoard extension and launch a session pointing to the log directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a489a432",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Tensorboard Extension\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b8fc54",
   "metadata": {},
   "source": [
    "## Visualizing Training Metrics\n",
    "\n",
    "TensorBoard displays graphs for **epoch accuracy** and **loss** for both training and validation datasets.  \n",
    "These visualizations help monitor model performance and detect **overfitting** or **underfitting**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ae7d10c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 30696), started 0:05:41 ago. (Use '!kill 30696' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-e89f5bd64b2cb20e\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-e89f5bd64b2cb20e\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir ../logs/fit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
