{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5df60248",
   "metadata": {},
   "source": [
    "# üìå Overview of Neural Network Architecture\n",
    "\n",
    "An **Artificial Neural Network (ANN)** typically consists of:\n",
    "- An **input layer**\n",
    "- One or more **hidden layers**\n",
    "- An **output layer**\n",
    "\n",
    "üëâ Example: A basic ANN might have **1 input layer, 2 hidden layers, and 1 output layer**.  \n",
    "Each layer contains **nodes (neurons)** interconnected with **weights** and **biases**.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Example ANN Structure\n",
    "- **Input Layer**: 2 inputs  \n",
    "- **Hidden Layer 1**: 3 neurons  \n",
    "- **Hidden Layer 2**: 2 neurons  \n",
    "- **Output Layer**: 1 neuron  \n",
    "\n",
    "Connections between layers are represented by **weight matrices** and **bias vectors**.\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Weight and Bias Dimensions\n",
    "\n",
    "1. **Input Layer (2 nodes) ‚Üí Hidden Layer 1 (3 neurons)**\n",
    "   - Weight matrix: `2 √ó 3` ‚Üí **6 weights**\n",
    "   - Bias terms: **3**\n",
    "\n",
    "2. **Hidden Layer 1 (3 neurons) ‚Üí Hidden Layer 2 (2 neurons)**\n",
    "   - Weight matrix: `3 √ó 2` ‚Üí **6 weights**\n",
    "   - Bias terms: **2**\n",
    "\n",
    "3. **Hidden Layer 2 (2 neurons) ‚Üí Output Layer (1 neuron)**\n",
    "   - Weight matrix: `2 √ó 1` ‚Üí **2 weights**\n",
    "   - Bias terms: **1**\n",
    "\n",
    "‚úÖ **Total trainable parameters**  \n",
    "`6 + 3 + 6 + 2 + 2 + 1 = 20`\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Building the ANN Model with TensorFlow Keras\n",
    "\n",
    "In **TensorFlow Keras**, the ANN is implemented as a **Sequential model** where layers are stacked one after another.  \n",
    "The `Dense` class is used to create **fully connected layers** (hidden neurons).\n",
    "\n",
    "### üîë Key Steps:\n",
    "1. **Initialize** a sequential model.  \n",
    "2. **Add Dense layers** with a specified number of neurons.  \n",
    "3. **Apply activation functions** (e.g., ReLU for hidden layers, sigmoid/softmax for output).  \n",
    "4. **Specify input shape** for the first hidden layer.  \n",
    "5. **Choose an optimizer** (e.g., Adam, SGD).  \n",
    "6. **Select a loss function** (e.g., binary crossentropy, categorical crossentropy).  \n",
    "7. **Define metrics** to evaluate model performance.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb4b22f",
   "metadata": {},
   "source": [
    "## Importing Required Libraries\n",
    "\n",
    "We import **TensorFlow** and relevant **Keras modules**:\n",
    "\n",
    "- `Sequential` from `tensorflow.keras.models`  \n",
    "  ‚ûù Used to initialize the Sequential model.  \n",
    "\n",
    "- `Dense` from `tensorflow.keras.layers`  \n",
    "  ‚ûù Used to create fully connected (dense) layers.  \n",
    "\n",
    "- `EarlyStopping` and `TensorBoard` from `tensorflow.keras.callbacks`  \n",
    "  ‚ûù For training control and visualization.  \n",
    "\n",
    "- `datetime`  \n",
    "  ‚ûù For timestamping log files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13324d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\RahulTejwani\\Downloads\\My_Files\\GenAI_Repo\\GenAI\\ANN Project Implementation\\venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## ANN Implementation\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e6ac27",
   "metadata": {},
   "source": [
    "### To run a notebook inside another notebook use\n",
    "\n",
    "```bash\n",
    "%run notebook1.ipynb\n",
    "```\n",
    "\n",
    "#### To use %run command we will need nbfromat library\n",
    "```bash\n",
    "pip install nbformat\n",
    "```\n",
    "\n",
    "- Now if we use %run notebook1.ipynb it will print all the outputs and the print statements in the notebook1.\n",
    "- if we don't want to see the output of notebook1 we can use following\n",
    "\n",
    "```bash\n",
    "%%capture cap\n",
    "%%run notebook1.ipynb\n",
    "\n",
    "```\n",
    "- capture will store the stdout and stderr in variable named cap. We can the see the output and the errors from notebook1 later using \n",
    "\n",
    "```python\n",
    "print(cap.stdout)\n",
    "print(cap.stderr)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46a40fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap\n",
    "%run 2_preprocessing.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "837720f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 12)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0448fc9f",
   "metadata": {},
   "source": [
    "## Constructing the Sequential Model\n",
    "\n",
    "We initialize the **Sequential model** and add layers:\n",
    "\n",
    "- **First hidden layer**  \n",
    "  - 64 neurons  \n",
    "  - ReLU activation  \n",
    "  - Specifies the input shape based on the training data features  \n",
    "\n",
    "- **Second hidden layer**  \n",
    "  - 32 neurons  \n",
    "  - ReLU activation  \n",
    "\n",
    "- **Output layer**  \n",
    "  - 1 neuron  \n",
    "  - Sigmoid activation (for binary classification)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b341ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\RahulTejwani\\Downloads\\My_Files\\GenAI_Repo\\GenAI\\ANN Project Implementation\\venv\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## BUild ANN model\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),  ## first hidden layer woth 64 neurons, input_shape is only needed in the first hidden layer\n",
    "    Dense(32, activation='relu'), # HL2\n",
    "    Dense(1, activation='sigmoid')  ## output layer for binary classification\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebd8ccec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 64)                832       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2945 (11.50 KB)\n",
      "Trainable params: 2945 (11.50 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## this is used to get the summary of model like trainable parameters.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b31654",
   "metadata": {},
   "source": [
    "## Optimizer in Model Training (TensorFlow/Keras)\n",
    "\n",
    "An **optimizer** is an algorithm that updates the **weights and biases** of a neural network during training to minimize the **loss function**.\n",
    "\n",
    "üëâ In simple terms:  \n",
    "- The **loss function** tells *how wrong* the model is.  \n",
    "- The **optimizer** decides *how to adjust the weights* to reduce that wrongness.  \n",
    "\n",
    "---\n",
    "\n",
    "### Common Optimizers in TensorFlow\n",
    "1. **SGD (Stochastic Gradient Descent)** ‚Äì Basic optimizer, updates weights using gradients.  \n",
    "2. **Momentum** ‚Äì Improves SGD by accelerating in relevant directions.  \n",
    "3. **Adam (Adaptive Moment Estimation)** ‚Äì Most popular, adapts learning rate for each parameter.  \n",
    "4. **RMSprop** ‚Äì Good for recurrent neural networks, adapts learning rate dynamically.  \n",
    "\n",
    "---\n",
    "\n",
    "### Example in Keras\n",
    "```python\n",
    "model.compile(\n",
    "    optimizer='adam',           # optimizer\n",
    "    loss='binary_crossentropy', # loss function\n",
    "    metrics=['accuracy']        # evaluation metric\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f61c2157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\RahulTejwani\\Downloads\\My_Files\\GenAI_Repo\\GenAI\\ANN Project Implementation\\venv\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## compile the model\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef411277",
   "metadata": {},
   "source": [
    "## TensorBoard\n",
    "\n",
    "**TensorBoard** is a visualization tool provided by TensorFlow.  \n",
    "It helps you **monitor, inspect, and debug** the training process of your neural networks.\n",
    "\n",
    "---\n",
    "\n",
    "### What TensorBoard Can Do\n",
    "1. **Track Metrics** ‚Äì like loss, accuracy, precision, recall over time.  \n",
    "2. **Visualize Model Graphs** ‚Äì see the layers and connections in your neural network.  \n",
    "3. **Histogram & Distributions** ‚Äì monitor weight and bias changes during training.  \n",
    "4. **Embedding Projector** ‚Äì visualize high-dimensional data (e.g., word embeddings).  \n",
    "5. **Scalars Dashboard** ‚Äì see training/validation curves easily.\n",
    "\n",
    "---\n",
    "\n",
    "### Example Usage\n",
    "```python\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import datetime\n",
    "\n",
    "# Create a log directory with timestamp\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# Add callback during training\n",
    "model.fit(X_train, y_train, \n",
    "          epochs=10, \n",
    "          validation_data=(X_test, y_test), \n",
    "          callbacks=[tensorboard_callback])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f735c7f",
   "metadata": {},
   "source": [
    "## Setting Up TensorBoard and EarlyStopping Callbacks\n",
    "\n",
    "- **TensorBoard** is used to visualize training logs and metrics.  \n",
    "- **EarlyStopping** monitors validation loss and stops training if no improvement occurs for a specified number of epochs (`patience`).  \n",
    "- **EarlyStopping** is a technique used during model training to prevent overfitting.\n",
    "    - It monitors a chosen metric (like validation loss or accuracy).\n",
    "\n",
    "    - If the metric doesn‚Äôt improve for a set number of epochs (patience), training stops early.\n",
    "\n",
    "    - It can also restore the best weights achieved during training (so the model doesn‚Äôt end up with worse weights from later epochs).\n",
    "\n",
    "    - üëâ Example:\n",
    "        If you set patience=3 and validation loss hasn‚Äôt improved for 3 consecutive epochs, training stops automatically.\n",
    "\n",
    "---\n",
    "\n",
    "### Example Setup\n",
    "```python\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping\n",
    "import datetime\n",
    "\n",
    "# TensorBoard log directory\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# EarlyStopping to avoid overfitting\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\", \n",
    "    patience=3, \n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Add callbacks during training\n",
    "model.fit(X_train, y_train,\n",
    "          epochs=20,\n",
    "          validation_data=(X_test, y_test),\n",
    "          callbacks=[tensorboard_callback, early_stopping_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28c6a6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up tensorboard\n",
    "\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "log_dir = \"../logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorflow_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85d163d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting up Earlystopping\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9524452d",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "We train the model using **`model.fit()`** by providing:\n",
    "\n",
    "- **Training data**: `(X_train, y_train)`  \n",
    "- **Validation data**: `(X_test, y_test)` to monitor performance  \n",
    "- **Number of epochs**: e.g., `100`  \n",
    "- **Callbacks**: for TensorBoard and EarlyStopping  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "308af1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:From c:\\Users\\RahulTejwani\\Downloads\\My_Files\\GenAI_Repo\\GenAI\\ANN Project Implementation\\venv\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\RahulTejwani\\Downloads\\My_Files\\GenAI_Repo\\GenAI\\ANN Project Implementation\\venv\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "250/250 [==============================] - 2s 3ms/step - loss: 0.4573 - accuracy: 0.8045 - val_loss: 0.3955 - val_accuracy: 0.8320\n",
      "Epoch 2/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3817 - accuracy: 0.8464 - val_loss: 0.3571 - val_accuracy: 0.8535\n",
      "Epoch 3/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.3525 - accuracy: 0.8556 - val_loss: 0.3474 - val_accuracy: 0.8610\n",
      "Epoch 4/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.3426 - accuracy: 0.8590 - val_loss: 0.3459 - val_accuracy: 0.8610\n",
      "Epoch 5/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3370 - accuracy: 0.8597 - val_loss: 0.3430 - val_accuracy: 0.8575\n",
      "Epoch 6/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3331 - accuracy: 0.8637 - val_loss: 0.3398 - val_accuracy: 0.8615\n",
      "Epoch 7/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3312 - accuracy: 0.8633 - val_loss: 0.3374 - val_accuracy: 0.8620\n",
      "Epoch 8/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3272 - accuracy: 0.8631 - val_loss: 0.3470 - val_accuracy: 0.8620\n",
      "Epoch 9/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3263 - accuracy: 0.8616 - val_loss: 0.3378 - val_accuracy: 0.8615\n",
      "Epoch 10/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3233 - accuracy: 0.8661 - val_loss: 0.3385 - val_accuracy: 0.8640\n",
      "Epoch 11/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3222 - accuracy: 0.8677 - val_loss: 0.3389 - val_accuracy: 0.8635\n",
      "Epoch 12/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3205 - accuracy: 0.8681 - val_loss: 0.3398 - val_accuracy: 0.8570\n",
      "Epoch 13/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3193 - accuracy: 0.8659 - val_loss: 0.3376 - val_accuracy: 0.8605\n",
      "Epoch 14/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3177 - accuracy: 0.8670 - val_loss: 0.3385 - val_accuracy: 0.8615\n",
      "Epoch 15/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3170 - accuracy: 0.8681 - val_loss: 0.3345 - val_accuracy: 0.8615\n",
      "Epoch 16/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3164 - accuracy: 0.8677 - val_loss: 0.3399 - val_accuracy: 0.8625\n",
      "Epoch 17/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3136 - accuracy: 0.8685 - val_loss: 0.3378 - val_accuracy: 0.8655\n",
      "Epoch 18/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3129 - accuracy: 0.8706 - val_loss: 0.3385 - val_accuracy: 0.8595\n",
      "Epoch 19/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3108 - accuracy: 0.8685 - val_loss: 0.3395 - val_accuracy: 0.8605\n",
      "Epoch 20/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3095 - accuracy: 0.8696 - val_loss: 0.3384 - val_accuracy: 0.8610\n",
      "Epoch 21/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3087 - accuracy: 0.8705 - val_loss: 0.3387 - val_accuracy: 0.8625\n",
      "Epoch 22/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3075 - accuracy: 0.8692 - val_loss: 0.3388 - val_accuracy: 0.8625\n",
      "Epoch 23/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3060 - accuracy: 0.8729 - val_loss: 0.3399 - val_accuracy: 0.8565\n",
      "Epoch 24/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3048 - accuracy: 0.8744 - val_loss: 0.3450 - val_accuracy: 0.8655\n",
      "Epoch 25/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3045 - accuracy: 0.8731 - val_loss: 0.3402 - val_accuracy: 0.8615\n"
     ]
    }
   ],
   "source": [
    "## Train the model\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train, validation_data= (X_test, y_test), epochs = 100,\n",
    "    callbacks=[ tensorflow_callback, early_stopping_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893e8370",
   "metadata": {},
   "source": [
    "## Observing Training Results\n",
    "\n",
    "- Training stops early if **validation loss** does not improve for the specified **patience**.  \n",
    "- **Accuracy** and **loss metrics** are displayed per epoch.  \n",
    "- **TensorBoard logs** are saved for visualization.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52559591",
   "metadata": {},
   "source": [
    "## Saving the Trained Model\n",
    "\n",
    "- After training, the model can be **saved** to an `.h5` file.  \n",
    "- This format is compatible with **Keras** and allows the model to be **loaded later** for inference or further training.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4845f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RahulTejwani\\Downloads\\My_Files\\GenAI_Repo\\GenAI\\ANN Project Implementation\\venv\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# save the model\n",
    "model.save(\"../models/ann_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6c94d7",
   "metadata": {},
   "source": [
    "## Launching TensorBoard\n",
    "- To visualize training logs, load the TensorBoard extension and launch a session pointing to the log directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a489a432",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Tensorboard Extension\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b8fc54",
   "metadata": {},
   "source": [
    "## Visualizing Training Metrics\n",
    "\n",
    "TensorBoard displays graphs for **epoch accuracy** and **loss** for both training and validation datasets.  \n",
    "These visualizations help monitor model performance and detect **overfitting** or **underfitting**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae7d10c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Failed to start `tensorboard`: [WinError 5] Access is denied"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir ../logs/fit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
