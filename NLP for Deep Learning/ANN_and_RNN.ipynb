{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b44e5294",
   "metadata": {},
   "source": [
    "### Why ANN Cannot Be Used for Sequential Data (Sentiment Analysis Example)\n",
    "\n",
    "**Artificial Neural Networks (ANNs)** are good at handling **tabular data** (features that are independent of order).  \n",
    "However, in **sequential data** (like text, speech, or time-series), the **order of inputs matters**.  \n",
    "ANNs treat inputs as fixed-length vectors without considering order, so they fail to capture context.\n",
    "\n",
    "---\n",
    "\n",
    "#### Problem Statement: Sentiment Analysis  \n",
    "Goal: Predict whether a movie review is **Positive** ðŸ˜Š or **Negative** ðŸ˜ž.  \n",
    "\n",
    "Example:  \n",
    "- Review 1: *\"The movie is **good**\"* â†’ Positive  \n",
    "- Review 2: *\"The movie is **not good**\"* â†’ Negative  \n",
    "\n",
    "---\n",
    "\n",
    "#### Why ANN Fails\n",
    "- ANN sees inputs as independent features:  \n",
    "  - `[\"The\", \"movie\", \"is\", \"good\"]` â†’ treated as just a set of words.  \n",
    "  - If order is shuffled â†’ `[\"good\", \"is\", \"movie\", \"The\"]`, ANN gives the same result.  \n",
    "- In reality, **sequence changes meaning**:  \n",
    "  - *\"not good\"* â‰  *\"good not\"*  \n",
    "  - Context matters, but ANN cannot remember previous words.  \n",
    "\n",
    "---\n",
    "\n",
    "#### Key Point\n",
    "- **ANN cannot capture sequential dependencies** (past words affecting current meaning).  \n",
    "- Therefore, ANN is not suitable for sentiment analysis or any problem where **order/sequence matters**.  \n",
    "\n",
    "\n",
    "#### Example by coverting words in vector using BOW\n",
    "\n",
    "#### ðŸ”¹ Step 1:Convert Text to Vectors using BoW (Bag of Words)\n",
    "- In **BoW**, we build a vocabulary of all unique words in our dataset.  \n",
    "- Each sentence is represented as a vector showing **word counts/presence**.  \n",
    "- **Order of words is lost.**\n",
    "---Example Vocabulary:  \n",
    "[\"The\", \"movie\", \"is\", \"not\", \"good\"]\n",
    "\n",
    "\n",
    "BoW vectors:  \n",
    "- *\"The movie is good\"* â†’ `[1, 1, 1, 0, 1]`  \n",
    "- *\"The movie is not good\"* â†’ `[1, 1, 1, 1, 1]`  \n",
    "- *\"good not movie The is\"* â†’ `[1, 1, 1, 1, 1]`  \n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ”¹ Step 2: How ANN Sees It\n",
    "- ANN takes these BoW vectors as input.  \n",
    "- Since BoW ignores order, ANN **cannot differentiate** between:  \n",
    "  - *\"The movie is not good\"* (Negative)  \n",
    "  - *\"The movie is good\"* (Positive)  \n",
    "- Both appear similar because the vectors just count words.  \n",
    "\n",
    "\n",
    "#### ANN Architechture:\n",
    "\n",
    "![ANN Diagram](images/ANN_diagram.png)\n",
    "---\n",
    "\n",
    "#### ðŸ”¹ Step 3: Why This is a Problem\n",
    "- In sentiment analysis, **sequence matters**:  \n",
    "  - \"good\" â†’ Positive  \n",
    "  - \"not good\" â†’ Negative  \n",
    "- BoW + ANN cannot capture this dependency because:  \n",
    "  - It treats all words independently.  \n",
    "  - It ignores context and order.  \n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ”¹ Step 4: Key Takeaway\n",
    "- **ANN with BoW** fails for sequential data because **order is lost**.  \n",
    "- For sentiment analysis and similar tasks, we need models that handle sequences:  \n",
    "  - **RNN (Recurrent Neural Network)**  \n",
    "  - **LSTM (Long Short-Term Memory)**  \n",
    "  - **GRU (Gated Recurrent Unit)**  \n",
    "  - **Transformers (e.g., BERT, GPT)**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e023f53d",
   "metadata": {},
   "source": [
    "### Solving Sentiment Analysis with Simple RNN\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ”¹ Problem Recap\n",
    "Using **BoW + ANN**, we lost the **sequence information** in text.  \n",
    "- *\"The movie is good\"* â†’ Positive  \n",
    "- *\"The movie is not good\"* â†’ Negative  \n",
    "Both looked almost the same in vector form.  \n",
    "\n",
    "We need a model that can **remember word order**.  \n",
    "This is where **Recurrent Neural Networks (RNNs)** help.\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ”¹ How RNN Works\n",
    "- RNN processes input **one word at a time** in sequence.  \n",
    "- At each step, it updates a **hidden state (memory)** based on:  \n",
    "  - Current word  \n",
    "  - Previous hidden state  \n",
    "\n",
    "Equations:  \n",
    "\\[\n",
    "h_t = f(Wx_t + Uh_{t-1} + b)\n",
    "\\]  \n",
    "\\[\n",
    "y_t = g(Vh_t)\n",
    "\\]\n",
    "\n",
    "- `x_t` = input at time `t` (word vector)  \n",
    "- `h_t` = hidden state (memory of sequence so far)  \n",
    "- `y_t` = output (prediction)  \n",
    "\n",
    "---\n",
    "\n",
    "#### RNN Architecture\n",
    "![RNN Diagram](images/RNN_diagram.png)\n",
    "\n",
    "- It has feedback loop\n",
    "- Each neuron in the hidden layer passes the processed output to every other neuron in the hidden layer\n",
    "- In case of RNN, input is passed one a time\n",
    "\n",
    "#### ðŸ”¹ Example with Sentiment Analysis\n",
    "Sentence: *\"The movie is not good\"*  \n",
    "\n",
    "RNN processes step by step:  \n",
    "Step 1: \"The\" â†’ h1\n",
    "Step 2: \"movie\" â†’ h2 (remembers \"The movie\")\n",
    "Step 3: \"is\" â†’ h3 (remembers \"The movie is\")\n",
    "Step 4: \"not\" â†’ h4 (remembers \"The movie is not\")\n",
    "Step 5: \"good\" â†’ h5 (remembers full context)\n",
    "\n",
    "- Final hidden state **h5** is passed to a classifier.  \n",
    "- Model learns that \"not good\" â†’ Negative sentiment.  \n",
    "\n",
    "---\n",
    "\n",
    "#### RNN Architecture showing passing of feedback between neurons in the hidden layer\n",
    "![RNN_passing_feedback_to_neurons](images\\RNN_passing_feedback_to_neurons.png)\n",
    "\n",
    "#### ðŸ”¹ Why RNN Solves the Problem\n",
    "- Unlike ANN, RNN **retains order** of words.  \n",
    "- Captures **dependencies** between words (e.g., \"not\" changes the meaning of \"good\").  \n",
    "- Sequence matters â†’ correct sentiment prediction.  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
